{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device = \"cuda:0\"\n",
    "obs = torch.rand([13,1]).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 14])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Knowledge deficit\n",
    "from einops import rearrange\n",
    "\n",
    "class SinusoidalPosition(nn.Module):\n",
    "    \"\"\"Relative positional encoding\"\"\"\n",
    "    def __init__(self, dim, min_timescale = 2., max_timescale = 1e4):\n",
    "        super().__init__()\n",
    "        freqs = torch.arange(0, dim, min_timescale)\n",
    "        inv_freqs = max_timescale ** (-freqs / dim)\n",
    "        self.register_buffer('inv_freqs', inv_freqs)\n",
    "\n",
    "    def forward(self, seq_len):\n",
    "        seq = torch.arange(seq_len - 1, -1, -1.)\n",
    "        sinusoidal_inp = rearrange(seq, 'n -> n ()') * rearrange(self.inv_freqs, 'd -> () d')\n",
    "        pos_emb = torch.cat((sinusoidal_inp.sin(), sinusoidal_inp.cos()), dim = -1)\n",
    "        return pos_emb\n",
    "\n",
    "pos_embedding = SinusoidalPosition(13)\n",
    "embedding = pos_embedding(16)\n",
    "\n",
    "embedding.shape\n",
    "\n",
    "# x = embedding.deatch().cpu().clone().numpy()\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            head_size:int,\n",
    "            seq_len  :int,\n",
    "            emb_size :float,\n",
    "            dropout  :float\n",
    "            ):\n",
    "        super(Head).__init__()\n",
    "        self.key   = nn.Linear(emb_size,head_size,bias=False)\n",
    "        self.query = nn.Linear(emb_size,head_size,bias=False)\n",
    "        self.value = nn.Linear(emb_size,head_size,bias=False)\n",
    "        self.register_buffer('tril',torch.tril(torch.ones(seq_len,seq_len)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def foward(self,embed,mask):\n",
    "        # mask wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        # What's the T dim\n",
    "\n",
    "        key   = self.key(embed)\n",
    "        query = self.query(embed)\n",
    "\n",
    "        weights = query @ key * key.shape[0] ** (0.5)\n",
    "        weights = weights.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        weights = F.softmax(weights)\n",
    "        out = weights @ self.value(embed)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_heads:int,\n",
    "            emb_size :float,\n",
    "            seq_len  :int,\n",
    "            dropout  :float\n",
    "            ):\n",
    "        super(MultiHeadAttention).__init__()\n",
    "        head_size = emb_size // num_heads\n",
    "        assert (num_heads*head_size == emb_size), 'emb_size must be an integer multiple of num_heads'\n",
    "        \n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                Head(head_size,seq_len,emb_size,dropout) for _ in range(num_heads)\n",
    "                ]\n",
    "            )\n",
    "        self.proj    = nn.Linear(num_heads*head_size,emb_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,embed):\n",
    "        out = torch.cat([head(embed) for head in self.heads])\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            emb_size    :float,\n",
    "            ff_expansion:int = 4\n",
    "            ):\n",
    "        super(FeedForward).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(emb_size,emb_size*ff_expansion),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(emb_size*ff_expansion,emb_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self,input):\n",
    "        return self.net(input)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_heads   :int,\n",
    "            seq_len     :int,\n",
    "            emb_size    :float,\n",
    "            ff_expansion:int,\n",
    "            dropout     :float\n",
    "            ):\n",
    "        super(DecoderBlock).__init__()\n",
    "        self.attention = MultiHeadAttention(num_heads,emb_size,seq_len,dropout)\n",
    "        self.ffnet   = FeedForward(emb_size,ff_expansion)\n",
    "        self.lnorm_1 = nn.LayerNorm(emb_size,emb_size)\n",
    "        self.lnorm_2 = nn.LayerNorm(emb_size,emb_size)\n",
    "    \n",
    "    def forward(self,embed):\n",
    "        attention = self.attention(self.lnorm_1(embed))\n",
    "        attention = self.lnorm_2(embed + attention)\n",
    "        out = embed + self.ffnet(attention)\n",
    "        return out\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_layers  :int,\n",
    "            num_heads   :int,\n",
    "            seq_len     :int,\n",
    "            obs_len     :int,\n",
    "            emb_size    :float,\n",
    "            ff_expansion:int,\n",
    "            dropout     :float\n",
    "            ):\n",
    "        super(Decoder).__init__()\n",
    "        self.embedding = nn.Embedding(seq_len,emb_size)\n",
    "        self.pos_embedding = nn.Embedding(obs_len,emb_size)\n",
    "        self.decoders = nn.Sequential(\n",
    "            *[DecoderBlock(num_heads,seq_len,emb_size,ff_expansion,dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.lnorm  = nn.LayerNorm(emb_size)\n",
    "        self.fc_out = nn.Linear(emb_size,seq_len)seq_len\n",
    "    \n",
    "    def forward(self,x):\n",
    "        embed   = self.embedding(x)\n",
    "        indices = torch.rand(x.shape[1]).to(device)\n",
    "        encode  = self.pos_embedding(indices)\n",
    "        embed   = embed + encode\n",
    "        out = self.decoders(embed)\n",
    "        out = self.fc_out(self.lnorm(out))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
